{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce185120",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af219162",
   "metadata": {
    "papermill": {
     "duration": 0.006804,
     "end_time": "2025-03-28T17:35:08.291081",
     "exception": false,
     "start_time": "2025-03-28T17:35:08.284277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3cfc9",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c51236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:35:08.299109Z",
     "iopub.status.busy": "2025-03-28T17:35:08.298684Z",
     "iopub.status.idle": "2025-03-28T17:35:08.622918Z",
     "shell.execute_reply": "2025-03-28T17:35:08.622496Z"
    },
    "papermill": {
     "duration": 0.328112,
     "end_time": "2025-03-28T17:35:08.623586",
     "exception": true,
     "start_time": "2025-03-28T17:35:08.295474",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtabulate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tabulate\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Para quitar unos warnings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas only supports SQLAlchemy connectable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Para quitar unos warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas only supports SQLAlchemy connectable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e43f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Pruebas de funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda5d1c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Importacion de las Consultas SQL**\n",
    "\n",
    "Prueba para ver si se importan de forma correcta los arhivos que tenemos en la carpeta queries/.Solo tomamos aquellos que vamos a utilizar para nuestro modelo dimensional,es decir los que empiezan por dim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff9202",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()  \n",
    "queries_path = os.path.join(BASE_DIR, \"..\", \"queries\")\n",
    "\n",
    "\n",
    "dim_files = glob.glob(os.path.join(queries_path, \"dim_*.sql\"))\n",
    "\n",
    "if not dim_files:\n",
    "    print(\"ERROR: No se encontraron archivos que empiecen con 'dim_'.\")\n",
    "else:\n",
    "    print(f\"Encontrados {len(dim_files)} archivos:\")\n",
    "    for sql_file in dim_files:\n",
    "        with open(sql_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_content = f.read()\n",
    "        print(f\"Archivo: {os.path.basename(sql_file)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ce882",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Exportación Tablas Azure-Local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2665e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Conexiones y Rutas de las Consultas**\n",
    "\n",
    "Este código establece las conexiones necesarias para extraer las tablas de Azure SQL Database y utilizar esas tablas para ejecutar las consultas necesarias que crean las tablas de nuestro modelo dimensional. Estas se cargarán en una base de datos local (SSMS).\n",
    "\n",
    "- Servidores:\n",
    "  - Azure SQL Database: Servidor en la nube (tablas originales)\n",
    "  - SQL Server Local (SSMS): Servidor local donde se crean las tablas del modelo dimensional\n",
    "\n",
    "- Driver:\n",
    "  - Se utiliza el ODBC Driver 17 for SQL Server, que viene por defecto en Windows y permite conectar y ejecutar consultas en ambas instancias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f035a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Conexión Azure\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "# Consexión SQL Server Local\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\n",
    "\n",
    "# Rutas a las consutlas para modelo dimensional\n",
    "BASE_DIR = os.getcwd()\n",
    "queries_path = os.path.join(BASE_DIR, \"..\", \"queries\")\n",
    "dim_files = glob.glob(os.path.join(queries_path, \"dim_*.sql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e528e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Establecemos las conexiones con Azure y SSMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7890d4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn_azure = pyodbc.connect(azure_conn_str)\n",
    "conn_local = pyodbc.connect(local_conn_str)\n",
    "cursor_local = conn_local.cursor()\n",
    "print(\"Conexiones establecidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfefb0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**ETL**\n",
    "\n",
    "Este código realiza un proceso ETL completo para migrar datos desde Azure a SQL Server Local:\n",
    "\n",
    "- *Extract:*  \n",
    "  Se conecta a Azure SQL y, para cada archivo SQL en la carpeta \"queries\", se ejecuta la consulta para extraer los datos (en forma de DataFrame).\n",
    "\n",
    "- *Transform*  \n",
    "  Se limpian los datos (se reemplazan nulos por 0) y se ajustan los tipos de datos (por ejemplo, de float64 a float32 y de int64 a int32) para optimizar la inserción en SQL Server.\n",
    "\n",
    "- *Load*  \n",
    "  Se elimina la tabla local (si existe), se crea una nueva tabla en SQL Server Local usando una definición dinámica basada en el DataFrame, y se insertan los datos extraídos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e194",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resumen_tablas = []\n",
    "\n",
    "for sql_file in dim_files:\n",
    "    table_name = os.path.splitext(os.path.basename(sql_file))[0]\n",
    "    NEW_TABLE_NAME = table_name  \n",
    "    \n",
    "    # EXTRACT: Leemos  la consulta y extraemos datos desde Azure SQL\n",
    "    with open(sql_file, 'r', encoding='utf-8') as file:\n",
    "        sql_query = file.read()\n",
    "    df = pd.read_sql(sql_query, conn_azure)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"Tabla: {NEW_TABLE_NAME} sin datos, se omite.\")\n",
    "        continue\n",
    "    \n",
    "    filas_extracted, columnas_extracted = df.shape\n",
    "\n",
    "    # TRANSFORM: Tratamiento tipos datos\n",
    "\n",
    "    # Tratamiento nulos\n",
    "\n",
    "    # 1. Reemplazar celdas vacías o con espacios por NaN\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    # 2. Tratamiento personalizado para columnas categóricas y sensibles\n",
    "    if 'GENERO' in df.columns:\n",
    "        df['GENERO'] = df['GENERO'].fillna('NA')  # Evita 0 para análisis categórico en Power BI\n",
    "\n",
    "    if 'provincia' in df.columns:\n",
    "        df['provincia'] = df['provincia'].fillna('NA')  # Mejor para filtros y agrupaciones\n",
    "\n",
    "    if 'QUEJA' in df.columns:\n",
    "        df['QUEJA'] = df['QUEJA'].fillna('NA')  # Así se distingue de 'NO'\n",
    "\n",
    "    if 'lat' in df.columns:\n",
    "        df['lat'] = df['lat'].fillna('NA')  # Evita errores en mapas (0,0)\n",
    "\n",
    "    if 'lon' in df.columns:\n",
    "        df['lon'] = df['lon'].fillna('NA')\n",
    "\n",
    "    if 'Fecha_Ultima_Revision' in df.columns:\n",
    "        df['Fecha_Ultima_Revision'] = pd.to_datetime(df['Fecha_Ultima_Revision'], errors='coerce')  # Deja como NaT los nulos\n",
    "\n",
    "    # 3. Aplicar fillna(0) al resto del DataFrame\n",
    "    columnas_excluidas = ['lat', 'lon', 'Fecha_Ultima_Revision', 'GENERO', 'provincia', 'QUEJA']\n",
    "    columnas_a_rellenar = [col for col in df.columns if col not in columnas_excluidas]\n",
    "\n",
    "    df[columnas_a_rellenar] = df[columnas_a_rellenar].fillna(0)\n",
    "\n",
    "    # 4. Asegurarnos de que todas las columnas con formato de fecha sean datetime\n",
    "    columnas_fecha = ['Fecha_Ultima_Revision', 'Sales_Date', 'FIN_GARANTIA', 'BASE_DATE', 'Prod_date', 'Logistic_date']  # Añade más si es necesario\n",
    "\n",
    "    for col in columnas_fecha:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    #5. Tipos de datos numéricos\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    # LOAD: Carga de las tablas del modelo dimensional en BBDD local\n",
    "    cursor_local.execute(f\"DROP TABLE IF EXISTS {NEW_TABLE_NAME}\")  # Eliminamos tabla (si ya existe)\n",
    "    conn_local.commit()\n",
    "    print(f\"-Tabla {NEW_TABLE_NAME} creada correctamente en SQL Server Local.\")\n",
    "\n",
    "    # Creamos la tabla con los tipos de datos ajustados\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE {NEW_TABLE_NAME} (\n",
    "        {', '.join([\n",
    "            f'[{col}] DATE' if np.issubdtype(df[col].dtype, np.datetime64)\n",
    "            else f'[{col}] FLOAT' if df[col].dtype == np.float32 \n",
    "            else f'[{col}] INT' if df[col].dtype == np.int32 \n",
    "            else f'[{col}] NVARCHAR(255)' for col in df.columns\n",
    "        ])}\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor_local.execute(create_table_sql)\n",
    "    conn_local.commit()\n",
    "    \n",
    "    # Insertamos datos en la tabla\n",
    "    placeholders = ', '.join(['?' for _ in df.columns])  # Rellenamos con interrogaciones para luego insertar los datos\n",
    "    insert_sql = f\"INSERT INTO {NEW_TABLE_NAME} VALUES ({placeholders})\"\n",
    "\n",
    "    cursor_local.fast_executemany = True\n",
    "    cursor_local.executemany(insert_sql, df.values.tolist())  # Creamos una lista con los valores del df y los introducimos en las tablas\n",
    "    conn_local.commit()\n",
    "    filas_insertadas = df.shape[0]\n",
    "    \n",
    "    # Mostramos un resumen de los datos extraídos para el modelo dimensional\n",
    "    print(f\" Tabla: {NEW_TABLE_NAME}\")\n",
    "    print(f\"  Columnas extraídas: {columnas_extracted}\")\n",
    "    print(f\"  Filas extraídas: {filas_extracted}\")\n",
    "    print(f\"  Tabla eliminada (si existía): Sí\")\n",
    "    print(f\"  Tabla creada: Sí\")\n",
    "    print(f\"  Filas insertadas: {filas_insertadas}\\n\")\n",
    "    \n",
    "    resumen_tablas.append({\n",
    "        \"Tabla\": NEW_TABLE_NAME,\n",
    "        \"Columnas extraídas\": columnas_extracted,\n",
    "        \"Filas extraídas\": filas_extracted,\n",
    "        \"Tabla eliminada\": \"Sí\",\n",
    "        \"Tabla creada\": \"Sí\",\n",
    "        \"Filas insertadas\": filas_insertadas\n",
    "    })\n",
    "\n",
    "print(\"\\n Modelo dimensional creado correctamente!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fd968",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Resumen de las tablas importadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efb8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tabulate(resumen_tablas, headers=\"keys\", tablefmt=\"grid\", numalign=\"center\"))\n",
    "print(\"\\nProceso completado correctamente!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5cb762",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Comprobaciones Finales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9c75d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Es muy importante **mantener la consistencia** de los datos entre el origen (Azure ) y el destino (SSMS), ya que una pérdida de registros podría afectar directamente a los análisis posteriores.\n",
    "\n",
    "Por eso, realizamos una verificación de integridad, comparando el número de filas que tiene cada tabla en Azure con las tablas generadas localmente tras ejecutar las consultas del modelo dimensional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e771c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Realizaremos la compración  de esta forma:\n",
    "| **Local Table (SSMS)** | **Azure Table**             |\n",
    "|------------------------|-----------------------------|\n",
    "| [dbo].[fact_table]   | [DATAEX].[001_sales]      |\n",
    "| [dbo].[cliente]      | [DATAEX].[003_clientes]   |\n",
    "| [dbo].[prod]         | [DATAEX].[006_producto]   |\n",
    "| [dbo].[geog]        | [DATAEX].[011_tienda]     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3cfac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tablas a comparar\n",
    "tables = {\n",
    "    \"[dbo].[dim_fact]\": \"[DATAEX].[001_sales]\",\n",
    "    \"[dbo].[dim_cliente]\":    \"[DATAEX].[003_clientes]\",\n",
    "    \"[dbo].[dim_prod]\":       \"[DATAEX].[006_producto]\",\n",
    "    \"[dbo].[dim_geog]\":       \"[DATAEX].[011_tienda]\"\n",
    "}\n",
    "\n",
    "# Lista para guardar resultados\n",
    "resultados = []\n",
    "\n",
    "\n",
    "for local_table, azure_table in tables.items():\n",
    "        # Contar filas en Azure\n",
    "        azure_query = f\"SELECT COUNT(*) AS Total_Filas FROM {azure_table}\"\n",
    "        df_azure = pd.read_sql(azure_query, conn_azure)\n",
    "        azure_count = df_azure['Total_Filas'].iloc[0]\n",
    "\n",
    "        # Contar filas en Local\n",
    "        local_query = f\"SELECT COUNT(*) AS Total_Filas FROM {local_table}\"\n",
    "        df_local = pd.read_sql(local_query, conn_local)\n",
    "        local_count = df_local['Total_Filas'].iloc[0]\n",
    "\n",
    "        # Comparación\n",
    "        resultado = \"Coinciden\" if azure_count == local_count else \"No coinciden\"\n",
    "        nombre_tabla = local_table.split('.')[-1].replace('[', '').replace(']', '')\n",
    "        resultados.append([nombre_tabla, azure_count, local_count, resultado])\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar tabla con resultados\n",
    "print(\"\\nResumen de comparación de tablas:\\n\")\n",
    "print(tabulate(resultados, headers=[\"Tabla\", \"Azure SQL\", \"SQL Local\", \"Resultado\"], tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.340679,
   "end_time": "2025-03-28T17:35:08.739718",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/martinagarciagonzalez/gd_clv-14/etl/01_modelo_dimensional.ipynb",
   "output_path": "/Users/martinagarciagonzalez/gd_clv-14/etl/01_modelo_dimensional_output.ipynb",
   "parameters": {},
   "start_time": "2025-03-28T17:35:07.399039",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}